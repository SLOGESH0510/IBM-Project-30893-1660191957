{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/Flowers-Dataset.zip'"
      ],
      "metadata": {
        "id": "7pXse1t73V7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e25565-b741-407c-92d3-1dc4757ed6ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Flowers-Dataset.zip\n",
            "replace flowers/daisy/100080576_f52e8ee070_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Augmentation\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "srEdL2IWwheM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Passing training and testing data to variables\n",
        "\n",
        "xtrain = train_datagen.flow_from_directory('/content/flowers',target_size=(64,64),class_mode='categorical',batch_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrQJ4ZqNx73x",
        "outputId": "54a5609e-7832-449f-bd1a-6f35470e0760"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4317 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtest = test_datagen.flow_from_directory('/content/flowers',target_size=(64,64),class_mode='categorical',batch_size=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwxqALks2o42",
        "outputId": "7c18ebb8-f40d-442a-c6c1-9b6f1307f93a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4317 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating model\n",
        "#importing libraries\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense"
      ],
      "metadata": {
        "id": "ZTz1hSDt5pn8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding layers\n",
        "\n",
        "model = Sequential()\n",
        "#Convolution layer\n",
        "model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(64,64,3)))\n",
        "#Pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#Flatten\n",
        "model.add(Flatten())\n",
        "#Fully connected layers\n",
        "model.add(Dense(400,activation='relu'))   #Hidden layer 1\n",
        "model.add(Dense(200,activation='relu'))   #Hidden layer 2\n",
        "model.add(Dense(5,activation='softmax'))  #Output layer\n"
      ],
      "metadata": {
        "id": "ppUvPG1z6SM1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "VHrfPx8e8Lae"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model\n",
        "\n",
        "model.fit_generator(xtrain, steps_per_epoch=20, validation_data=xtest, validation_steps=len(xtest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYl9ljoA8h_N",
        "outputId": "cdde9006-97c9-404b-bcec-b81e0240c20e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 30s 2s/step - loss: 2.2688 - accuracy: 0.2815 - val_loss: 1.3726 - val_accuracy: 0.4077\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3694cc6550>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model\n",
        "\n",
        "model.save('Flower.h5')"
      ],
      "metadata": {
        "id": "r_dMKsmp88VL"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "img= image.load_img('/content/flowers/dandelion/11775820493_10fedf4bff_n.jpg',target_size=(64,64))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVpHecec9VXD",
        "outputId": "28222f06-1809-454a-c916-a975d0761ec6"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 86., 125., 182.],\n",
              "        [ 88., 127., 184.],\n",
              "        [ 88., 127., 184.],\n",
              "        ...,\n",
              "        [ 72., 111., 166.],\n",
              "        [ 71., 108., 163.],\n",
              "        [ 69., 108., 163.]],\n",
              "\n",
              "       [[ 87., 126., 183.],\n",
              "        [ 89., 127., 189.],\n",
              "        [ 90., 129., 186.],\n",
              "        ...,\n",
              "        [ 74., 113., 168.],\n",
              "        [ 74., 111., 164.],\n",
              "        [ 70., 110., 162.]],\n",
              "\n",
              "       [[ 89., 128., 185.],\n",
              "        [ 92., 128., 188.],\n",
              "        [ 91., 130., 187.],\n",
              "        ...,\n",
              "        [ 75., 115., 167.],\n",
              "        [ 74., 114., 166.],\n",
              "        [ 72., 111., 166.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 40.,  64.,   2.],\n",
              "        [ 38.,  59.,   2.],\n",
              "        [ 22.,  45.,  17.],\n",
              "        ...,\n",
              "        [  6.,  14.,   3.],\n",
              "        [ 11.,  31.,  20.],\n",
              "        [ 20.,  36.,  23.]],\n",
              "\n",
              "       [[ 33.,  56.,   4.],\n",
              "        [ 35.,  51.,   6.],\n",
              "        [ 21.,  41.,  13.],\n",
              "        ...,\n",
              "        [  1.,  11.,   2.],\n",
              "        [  0.,   8.,   0.],\n",
              "        [  8.,  22.,   9.]],\n",
              "\n",
              "       [[ 31.,  48.,   3.],\n",
              "        [ 22.,  37.,   6.],\n",
              "        [ 33.,  49.,   0.],\n",
              "        ...,\n",
              "        [  1.,  10.,   7.],\n",
              "        [  3.,   8.,   2.],\n",
              "        [  0.,   9.,   1.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.expand_dims(x,axis=0)\n",
        "\n",
        "#Predict\n",
        "model.predict(x)\n",
        "xtrain.class_indices\n",
        "op= ['daisy','dandelion','rose','sunflower','tulip']\n",
        "pred= np.argmax(model.predict(x))\n",
        "op[pred]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pqirwL91_MVc",
        "outputId": "9074ed42-e558-44c1-fc54-1e9e591c9a22"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dandelion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model tuning\n",
        "#import libraries\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "early_stopping = EarlyStopping(monitor='value_accuracy',patience=5)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='value_accuracy',patience=5, factor=0.5, min_lr=0.0001)\n",
        "callback= [reduce_lr, early_stopping]\n"
      ],
      "metadata": {
        "id": "d6v9aYiJ_-QQ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(xtrain, steps_per_epoch=100,callbacks= callback, validation_data=xtest, validation_steps=len(xtest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un6xmRV9A-7h",
        "outputId": "df0525f2-4632-4220-dbc0-eced3cfe4d90"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 44/100 [============>.................] - ETA: 37s - loss: 0.7350 - accuracy: 0.7227"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n",
            "WARNING:tensorflow:Learning rate reduction is conditioned on metric `value_accuracy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `value_accuracy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 45s 446ms/step - loss: 0.7350 - accuracy: 0.7227 - val_loss: 0.6672 - val_accuracy: 0.7433 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3694a2b390>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the model\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "img= image.load_img('/content/flowers/rose/10090824183_d02c613f10_m.jpg',target_size=(64,64))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g26Jr1GEBQW9",
        "outputId": "1ca0df14-761b-4462-f2f7-fe07ff0d1662"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[14., 22.,  7.],\n",
              "        [11., 22.,  6.],\n",
              "        [ 8., 19.,  3.],\n",
              "        ...,\n",
              "        [32., 47., 24.],\n",
              "        [30., 48., 22.],\n",
              "        [33., 49., 23.]],\n",
              "\n",
              "       [[13., 20., 12.],\n",
              "        [11., 21., 10.],\n",
              "        [11., 22.,  8.],\n",
              "        ...,\n",
              "        [37., 51., 26.],\n",
              "        [35., 49., 26.],\n",
              "        [25., 45., 20.]],\n",
              "\n",
              "       [[19., 30., 16.],\n",
              "        [19., 31., 17.],\n",
              "        [16., 29., 12.],\n",
              "        ...,\n",
              "        [31., 47., 20.],\n",
              "        [28., 49., 18.],\n",
              "        [27., 43., 17.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[15., 17.,  6.],\n",
              "        [ 2.,  9.,  2.],\n",
              "        [ 2.,  9.,  1.],\n",
              "        ...,\n",
              "        [ 8., 21., 11.],\n",
              "        [ 2., 12.,  3.],\n",
              "        [ 9., 16.,  9.]],\n",
              "\n",
              "       [[12., 20.,  9.],\n",
              "        [ 1.,  8.,  1.],\n",
              "        [ 5., 10.,  3.],\n",
              "        ...,\n",
              "        [ 3.,  8.,  2.],\n",
              "        [ 6., 16.,  5.],\n",
              "        [ 5.,  7.,  4.]],\n",
              "\n",
              "       [[24., 27., 18.],\n",
              "        [11., 21., 13.],\n",
              "        [ 8., 13.,  6.],\n",
              "        ...,\n",
              "        [ 1.,  6.,  0.],\n",
              "        [ 2.,  9.,  1.],\n",
              "        [ 2.,  9.,  1.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.expand_dims(x,axis=0)\n",
        "\n",
        "#Predict\n",
        "model.predict(x)\n",
        "xtrain.class_indices\n",
        "op= ['daisy','dandelion','rose','sunflower','tulip']\n",
        "pred= np.argmax(model.predict(x))\n",
        "op[pred]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hwLZxa_QCjji",
        "outputId": "701533fc-0cab-4191-82a3-3c9c32064e99"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'daisy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7AfICjTCsIv"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}